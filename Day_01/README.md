# Machine Learning

## Topics covered in today's module

* Introduction to Machine Learning
* Machine learning with Scikit-learn
* Decision Trees
* Linear Regression
* Random Forests

## Main takeaways from doing today's assignment

Learning the difference betwen Linear Regression, Random forests and Decision Trees

Linear Regression, Decision Trees, and Random Forests are all machine learning algorithms that can be used for supervised learning tasks such as regression and classification.

- Linear Regression is a simple algorithm that models the relationship between a dependent variable and one or more independent variables using a straight line.

- Decision Trees are a type of algorithm where a tree-like model of decisions and their possible consequences is developed to predict an outcome.

- Random Forests is an extension of decision trees. It builds multiple decision trees and merges them together to get a more accurate and stable prediction. In random forests, each tree in the ensemble is built from a random sample of the training data.

In summary:
- Linear Regression: Simple, best for linear relationships between variables, fast.
- Decision Trees: Easy to interpret, can handle non-linear relationships, prone to overfitting.
- Random Forests: Accurate, can handle non-linear relationships, less prone to overfitting, takes longer to train.

## Challenging, interesting, or exciting aspects of today's assignment
Using the RandomForestClassifier was challenging for question 5

## Additional resources used 
https://towardsdatascience.com/decision-trees-explained-entropy-information-gain-gini-index-ccp-pruning-4d78070db36c
