# Activation Functions

## Topics covered in today's module
* Introduction to Sigmoid, Tanh, ReLU
* Visualizing how ReLU affects the feature maps
* Vanishing and exploding gradients
* Dying ReLU problem
* Advanced activation functions: Swish, GeLU, SeLU

## Main takeaways from doing today's assignment

Max-pooling is used to reduce the spatial dimension of feature maps while retaining the most important information. Max-pooling works by partitioning the input image or feature map into non-overlapping regions and taking the maximum value from each region to produce a new, smaller feature map.

One of the consequences of this operation is that the resulting feature maps after max-pooling are more pixelated than the original feature maps. This happens because the pooling operation reduces the spatial resolution of the feature maps, and the resulting pixels in the pooled feature map represent a larger area of the input image.

## Challenging, interesting, or exciting aspects of today's assignment
Matching the activation functions to the graph lines.

## Additional resources used 
https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6
