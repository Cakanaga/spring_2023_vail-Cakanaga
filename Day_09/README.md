# Overfitting and Underfitting

## Topics covered in today's module
* Overfitting
* Underfitting
* Data preparation
* Reducing network capacity

## Main takeaways from doing today's assignment

From the regularization methods used so far, the main takeaways on reducing underfitting and reducing overfitting are:

L1 and L2 regularization can help reduce overfitting by adding a penalty term to the loss function that discourages the model from having large weights.

Dropout regularization can also help reduce overfitting by randomly dropping out a proportion of the neurons during training, forcing the model to learn more robust and generalizable representations.

Early stopping can help prevent overfitting by monitoring the validation loss during training and stopping the training process when the validation loss starts to increase.

Data augmentation can help reduce overfitting by artificially increasing the size of the training set through transformations such as rotation, flipping, and zooming.

Increasing model capacity and/or adding more layers can help reduce underfitting by allowing the model to learn more complex and expressive representations.

## Challenging, interesting, or exciting aspects of today's assignment
Understand loss/epoch in case of underfitting

## Additional resources used 
<To be filled>
